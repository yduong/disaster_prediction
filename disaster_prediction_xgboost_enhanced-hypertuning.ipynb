{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f395fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c91c646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker==1.72.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (1.72.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==0.1.4 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.4)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.5.3)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.15.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (20.9)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (3.7.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (0.1.5)\n",
      "Requirement already satisfied: boto3>=1.14.12 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.17.99)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from sagemaker==1.72.0) (1.19.5)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.4.2)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.99 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (1.20.99)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.14.12->sagemaker==1.72.0) (0.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.99->boto3>=1.14.12->sagemaker==1.72.0) (1.26.5)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from botocore<1.21.0,>=1.20.99->boto3>=1.14.12->sagemaker==1.72.0) (2.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.72.0) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.72.0) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.72.0) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/mxnet_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker==1.72.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f7c8b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eef973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3f8f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for datasets.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"\\'s\", \"\", string)\n",
    "    string = re.sub(r\"\\'ve\", \"\", string)\n",
    "    string = re.sub(r\"n\\'t\", \"\", string)\n",
    "    string = re.sub(r\"\\'re\", \"\", string)\n",
    "    string = re.sub(r\"\\'d\", \"\", string)\n",
    "    string = re.sub(r\"\\'ll\", \"\", string)\n",
    "    string = re.sub(r\",\", \"\", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \"\", string)\n",
    "    string = re.sub(r\"\\)\", \"\", string)\n",
    "    string = re.sub(r\"\\?\", \"\", string)\n",
    "    string = re.sub(r\"'\", \"\", string)\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"[0-9]\\w+|[0-9]\",\"\", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    \n",
    "    rm_links = re.sub(r\"http\\S+\", \" \", string)\n",
    "    rm_pn = re.sub(r'[^\\w\\s]+', ' ', rm_links)\n",
    "    rm_sc = re.sub('\\?|\\.|\\!|\\/|\\;|\\:', ' ', rm_pn)\n",
    "    tk = word_tokenize(rm_pn)\n",
    "    clean_text = [word.lower() for word in tk if word.lower() not in stop_words]\n",
    "    reformed_text = ' '.join(clean_text)\n",
    "    return reformed_text\n",
    "\n",
    "def review_to_words(review):\n",
    "    text = BeautifulSoup(review, \"html.parser\").get_text() # Remove HTML tags\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower()) # Convert to lower case\n",
    "    text = clean_text(text)\n",
    "    words = text.split() # Split string into words\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")] # Remove stopwords\n",
    "    words = ' '.join([PorterStemmer().stem(w) for w in words]) # stem\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f43858c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"clean_text\"] = train_data[\"text\"].apply(lambda row:review_to_words(row))\n",
    "test_data[\"clean_text\"] = test_data[\"text\"].apply(lambda row:review_to_words(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a1fefe",
   "metadata": {},
   "source": [
    "## use TfidfTransformer to generate feature for test and train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5326f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6090, 13643)\n",
      "(3263, 13643)\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('count', CountVectorizer()),\n",
    "                 ('tfid', TfidfTransformer())])\n",
    "\n",
    "## split data into train and evaluation set\n",
    "train, evaluation = train_test_split(train_data, test_size=0.2)\n",
    "\n",
    "train_feature = pipe.fit_transform(train[\"clean_text\"]).toarray()\n",
    "test_feature = pipe.transform(test_data[\"clean_text\"] ).toarray()\n",
    "evaluation_feature = pipe.transform(evaluation[\"clean_text\"]).toarray()\n",
    "\n",
    "\n",
    "print(train_feature.shape)\n",
    "print(test_feature.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b4544",
   "metadata": {},
   "source": [
    "## Save data to csv file and upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "316349f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "role = get_execution_role()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edb15a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '../data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "## save data to csv file\n",
    "train_df = pd.concat([train[\"target\"],pd.DataFrame(train_feature)], axis=1)\n",
    "train_df.to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)\n",
    "evaluation_df = pd.concat([evaluation[\"target\"],pd.DataFrame(evaluation_feature)], axis=1)\n",
    "evaluation_df.columns = train_df.columns\n",
    "evaluation_df.to_csv(os.path.join(data_dir, 'evaluation.csv'), header=False, index=False)\n",
    "\n",
    "pd.DataFrame(test_feature).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "\n",
    "# upload to s3\n",
    "prefix = 'predict-disaster'\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n",
    "evaluation_location = session.upload_data(os.path.join(data_dir, 'evaluation.csv'), key_prefix=prefix)\n",
    "test_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeb106f",
   "metadata": {},
   "source": [
    "## Train the model with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47540b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost', '1.0-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10e40a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                    role,                                    # What is our current IAM Role\n",
    "                                    train_instance_count=1,                  # How many compute instances\n",
    "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffc1e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "xgb_hyperparameter_tuner = HyperparameterTuner(estimator = xgb, # The estimator object to use as the basis for the training jobs.\n",
    "                                               objective_metric_name = 'validation:f1', # The metric used to compare trained models.\n",
    "                                               objective_type = 'Maximize', # Whether we wish to minimize or maximize the metric.\n",
    "                                               max_jobs = 6, # The total number of models to train\n",
    "                                               max_parallel_jobs = 3, # The number of models to train in parallel\n",
    "                                               hyperparameter_ranges = {\n",
    "                                                    'max_depth': IntegerParameter(3, 12),\n",
    "                                                    'eta'      : ContinuousParameter(0.05, 0.5),\n",
    "                                                    'min_child_weight': IntegerParameter(2, 8),\n",
    "                                                    'subsample': ContinuousParameter(0.5, 0.9),\n",
    "                                                    'gamma': ContinuousParameter(0, 10),\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eee2a8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=evaluation_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aa64ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "xgb_hyperparameter_tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})\n",
    "xgb_hyperparameter_tuner.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d163c",
   "metadata": {},
   "source": [
    "## Retrain model with best parameter and predict data on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce05d141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-01 15:39:04 Starting - Starting the training job...\n",
      "2021-08-01 15:39:08 Starting - Launching requested ML instances......\n",
      "2021-08-01 15:40:34 Starting - Preparing the instances for training............\n",
      "2021-08-01 15:42:08 Downloading - Downloading input data......\n",
      "2021-08-01 15:43:23 Training - Downloading the training image...\n",
      "2021-08-01 15:43:45 Training - Training image download completed. Training in progress.\u001b[34mINFO:sagemaker-containers:Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:Failed to parse hyperparameter objective value binary:logistic to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34mINFO:sagemaker-containers:No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34mINFO:sagemaker_xgboost_container.training:Running XGBoost Sagemaker in algorithm mode\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34mINFO:root:Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[15:43:54] 7613x15774 matrix with 120087462 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34mINFO:root:Single node training.\u001b[0m\n",
      "\u001b[34mINFO:root:Train matrix has 7613 rows\u001b[0m\n",
      "\u001b[34m[15:43:54] WARNING: /workspace/src/learner.cc:328: \u001b[0m\n",
      "\u001b[34mParameters: { early_stopping_rounds, num_round, silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.36924\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.35439\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.35597\u001b[0m\n",
      "\u001b[34m[3]#011train-error:0.34336\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.33127\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.33049\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.31919\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.30645\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.29804\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.29095\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.28688\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.28136\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.26901\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.26468\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.26074\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.25286\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.24747\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.24642\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.24235\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.24051\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.23736\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.23709\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.23552\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.23223\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.23039\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.22895\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.22790\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.22540\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.22422\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.22094\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.22028\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.21936\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.21949\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.21700\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.21674\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.21450\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.21411\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.21345\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.21463\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.21319\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.21227\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.21187\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.20977\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.20990\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.20951\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.20885\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.20833\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.20728\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.20610\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.20504\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.20426\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.20662\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.20413\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.20517\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.20426\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.20478\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.20294\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.20268\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.20268\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.20255\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.20229\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.20202\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.20163\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.20189\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.20045\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.20018\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.19966\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.19887\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.19900\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.19861\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.19874\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.19861\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.19782\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.19782\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.19795\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.19756\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.19729\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.19677\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.19598\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.19572\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.19532\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.19493\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.19545\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.19519\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.19572\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.19598\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.19664\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.19611\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.19611\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.19559\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.19519\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.19493\u001b[0m\n",
      "\n",
      "2021-08-01 15:45:48 Uploading - Uploading generated training model\n",
      "2021-08-01 15:45:48 Completed - Training job completed\n",
      "Training seconds: 220\n",
      "Billable seconds: 220\n"
     ]
    }
   ],
   "source": [
    "#xgb_attached = sagemaker.estimator.Estimator.attach(xgb_hyperparameter_tuner.best_training_job())\n",
    "#xgb_transformer = xgb_attached.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                    role,                                    # What is our current IAM Role\n",
    "                                    train_instance_count=1,                  # How many compute instances\n",
    "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "\n",
    "xgb.set_hyperparameters(max_depth=3,\n",
    "                        eta=0.4,\n",
    "                        gamma=6.5,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.78,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)\n",
    "\n",
    "train_feature = pipe.fit_transform(train_data[\"clean_text\"]).toarray()\n",
    "train_df = pd.concat([train_data[\"target\"],pd.DataFrame(train_feature)], axis=1)\n",
    "train_df.to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)\n",
    "\n",
    "train_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "xgb.fit({'train': s3_input_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52c54b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................................\u001b[34m[2021-08-01:15:52:23:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:23:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:23:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\n",
      "\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\n",
      "\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "\n",
      "    keepalive_timeout 3;\n",
      "\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-08-01 15:52:23 +0000] [17] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2021-08-01 15:52:23 +0000] [17] [INFO] Listening at: unix:/tmp/gunicorn.sock (17)\u001b[0m\n",
      "\u001b[34m[2021-08-01 15:52:23 +0000] [17] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-08-01 15:52:23 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[34m[2021-08-01 15:52:23 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[34m[2021-08-01 15:52:23 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2021-08-01 15:52:24 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:29:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Aug/2021:15:52:29 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:29:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Aug/2021:15:52:29 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n",
      "\u001b[32m2021-08-01T15:52:29.686:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:31:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-08-01:15:52:31:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2021-08-01:15:52:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-08-01:15:52:31:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-08-01:15:52:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:32:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m[2021-08-01:15:52:32:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2021-08-01:15:52:32:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:33:ERROR] Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[35m[2021-08-01:15:52:33:ERROR] Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 220, in invocations\n",
      "    preds = ScoringService.predict(data=dtest, content_type=content_type, model_format=format)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 84, in predict\n",
      "    return serve_utils.predict(cls.booster, model_format, data, content_type)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve_utils.py\", line 163, in predict\n",
      "    format(content_type, y, x))\u001b[0m\n",
      "\u001b[34mValueError: Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Aug/2021:15:52:33 +0000] \"POST /invocations HTTP/1.1\" 400 137 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:33:ERROR] Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 220, in invocations\n",
      "    preds = ScoringService.predict(data=dtest, content_type=content_type, model_format=format)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 84, in predict\n",
      "    return serve_utils.predict(cls.booster, model_format, data, content_type)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve_utils.py\", line 163, in predict\n",
      "    format(content_type, y, x))\u001b[0m\n",
      "\u001b[34mValueError: Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Aug/2021:15:52:33 +0000] \"POST /invocations HTTP/1.1\" 400 137 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:34:ERROR] Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 220, in invocations\n",
      "    preds = ScoringService.predict(data=dtest, content_type=content_type, model_format=format)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 84, in predict\n",
      "    return serve_utils.predict(cls.booster, model_format, data, content_type)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve_utils.py\", line 163, in predict\n",
      "    format(content_type, y, x))\u001b[0m\n",
      "\u001b[34mValueError: Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Aug/2021:15:52:34 +0000] \"POST /invocations HTTP/1.1\" 400 137 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-08-01:15:52:34:ERROR] Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 220, in invocations\n",
      "    preds = ScoringService.predict(data=dtest, content_type=content_type, model_format=format)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 84, in predict\n",
      "    return serve_utils.predict(cls.booster, model_format, data, content_type)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve_utils.py\", line 163, in predict\n",
      "    format(content_type, y, x))\u001b[0m\n",
      "\u001b[34mValueError: Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Aug/2021:15:52:34 +0000] \"POST /invocations HTTP/1.1\" 400 137 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 220, in invocations\n",
      "    preds = ScoringService.predict(data=dtest, content_type=content_type, model_format=format)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 84, in predict\n",
      "    return serve_utils.predict(cls.booster, model_format, data, content_type)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve_utils.py\", line 163, in predict\n",
      "    format(content_type, y, x))\u001b[0m\n",
      "\u001b[35mValueError: Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Aug/2021:15:52:33 +0000] \"POST /invocations HTTP/1.1\" 400 137 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-08-01:15:52:33:ERROR] Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 220, in invocations\n",
      "    preds = ScoringService.predict(data=dtest, content_type=content_type, model_format=format)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 84, in predict\n",
      "    return serve_utils.predict(cls.booster, model_format, data, content_type)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve_utils.py\", line 163, in predict\n",
      "    format(content_type, y, x))\u001b[0m\n",
      "\u001b[35mValueError: Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Aug/2021:15:52:33 +0000] \"POST /invocations HTTP/1.1\" 400 137 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-08-01:15:52:34:ERROR] Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 220, in invocations\n",
      "    preds = ScoringService.predict(data=dtest, content_type=content_type, model_format=format)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 84, in predict\n",
      "    return serve_utils.predict(cls.booster, model_format, data, content_type)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve_utils.py\", line 163, in predict\n",
      "    format(content_type, y, x))\u001b[0m\n",
      "\u001b[35mValueError: Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Aug/2021:15:52:34 +0000] \"POST /invocations HTTP/1.1\" 400 137 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-08-01:15:52:34:ERROR] Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 220, in invocations\n",
      "    preds = ScoringService.predict(data=dtest, content_type=content_type, model_format=format)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve.py\", line 84, in predict\n",
      "    return serve_utils.predict(cls.booster, model_format, data, content_type)\n",
      "  File \"/miniconda3/lib/python3.6/site-packages/sagemaker_xgboost_container/algorithm_mode/serve_utils.py\", line 163, in predict\n",
      "    format(content_type, y, x))\u001b[0m\n",
      "\u001b[35mValueError: Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Aug/2021:15:52:34 +0000] \"POST /invocations HTTP/1.1\" 400 137 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:33.939:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: ClientError: 400\u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:33.939:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: \u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:33.939:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: Message:\u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:33.939:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: Unable to evaluate payload provided: Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:33.942:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: ClientError: 400\u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:33.942:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: \u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:33.942:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: Message:\u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:33.942:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: Unable to evaluate payload provided: Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:34.103:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: ClientError: 400\u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:34.103:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: \u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:34.103:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: Message:\u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:34.103:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: Unable to evaluate payload provided: Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:34.186:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: ClientError: 400\u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:34.186:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: \u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:34.186:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: Message:\u001b[0m\n",
      "\u001b[32m2021-08-01T15:52:34.187:[sagemaker logs]: sagemaker-us-east-1-099005516989/predict-disaster/test.csv: Unable to evaluate payload provided: Feature size of csv inference data 13643 is not consistent with feature size of trained model 15774.\u001b[0m\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Transform job sagemaker-xgboost-2021-08-01-15-47-13-554: Failed. Reason: ClientError: See job logs for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-97c31d5fdb6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mxgb_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstance_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ml.m4.xlarge'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mxgb_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text/csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Line'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mxgb_transformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/transformer.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;34m\"\"\"Placeholder docstring\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_last_transform_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_transform_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop_transform_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/transformer.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_transform_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_transform_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_transform_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   3245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3247\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TransformJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3248\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3249\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                 ),\n\u001b[1;32m   2670\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2672\u001b[0m             )\n\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Transform job sagemaker-xgboost-2021-08-01-15-47-13-554: Failed. Reason: ClientError: See job logs for more information"
     ]
    }
   ],
   "source": [
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf9b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir\n",
    "\n",
    "prediction = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "prediction = [round(num) for num in prediction.squeeze().values]\n",
    "\n",
    "predict = pd.DataFrame(prediction)\n",
    "predict[\"id\"] = test_data[\"id\"].values\n",
    "predict[\"target\"] = prediction\n",
    "predict[[\"id\",\"target\"]].to_csv(\"predict_boostrap_hyptertuning.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca56815",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The result is submitted to Kaggle, the score is 0.782"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41131c2",
   "metadata": {},
   "source": [
    "## Manually Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eafc604f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The weather is nice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The storm is coming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>There is no storm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  text\n",
       "0  The weather is nice\n",
       "1  The storm is coming\n",
       "2    There is no storm"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_df = pd.read_csv(\"manual_test.csv\",header=None)\n",
    "manual_df.columns = [\"text\"]\n",
    "manual_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64642178",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_df[\"clean_text\"] = manual_df[\"text\"].apply(lambda row:review_to_words(row))\n",
    "manual_test_feature = pipe.transform(manual_df[\"clean_text\"] ).toarray()\n",
    "pd.DataFrame(manual_test_feature).to_csv(os.path.join(data_dir, 'manul_test.csv'), header=False, index=False)\n",
    "manul_test_location = session.upload_data(os.path.join(data_dir, 'manul_test.csv'), key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ef36ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "Using already existing model: sagemaker-xgboost-2021-08-01-15-39-04-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................\u001b[34m[2021-08-01:16:06:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-08-01:16:06:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-08-01:16:06:38:INFO] nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\n",
      "\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\n",
      "\u001b[0m\n",
      "\u001b[35m[2021-08-01:16:06:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2021-08-01:16:06:38:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2021-08-01:16:06:38:INFO] nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\n",
      "\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\n",
      "\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "\n",
      "    keepalive_timeout 3;\n",
      "\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m[2021-08-01 16:06:38 +0000] [17] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "\n",
      "    keepalive_timeout 3;\n",
      "\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35m[2021-08-01 16:06:38 +0000] [17] [INFO] Starting gunicorn 19.10.0\u001b[0m\n",
      "\u001b[34m[2021-08-01 16:06:38 +0000] [17] [INFO] Listening at: unix:/tmp/gunicorn.sock (17)\u001b[0m\n",
      "\u001b[34m[2021-08-01 16:06:38 +0000] [17] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-08-01 16:06:38 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[34m[2021-08-01 16:06:38 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[34m[2021-08-01 16:06:38 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2021-08-01 16:06:38 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[35m[2021-08-01 16:06:38 +0000] [17] [INFO] Listening at: unix:/tmp/gunicorn.sock (17)\u001b[0m\n",
      "\u001b[35m[2021-08-01 16:06:38 +0000] [17] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2021-08-01 16:06:38 +0000] [24] [INFO] Booting worker with pid: 24\u001b[0m\n",
      "\u001b[35m[2021-08-01 16:06:38 +0000] [25] [INFO] Booting worker with pid: 25\u001b[0m\n",
      "\u001b[35m[2021-08-01 16:06:38 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[35m[2021-08-01 16:06:38 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m[2021-08-01:16:06:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Aug/2021:16:06:44 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m[2021-08-01:16:06:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Aug/2021:16:06:44 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-08-01:16:06:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Aug/2021:16:06:44 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-08-01:16:06:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Aug/2021:16:06:44 +0000] \"GET /execution-parameters HTTP/1.1\" 200 84 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2021-08-01T16:06:44.403:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\u001b[34m[2021-08-01:16:06:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2021-08-01:16:06:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Aug/2021:16:06:44 +0000] \"POST /invocations HTTP/1.1\" 200 57 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m[2021-08-01:16:06:44:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m[2021-08-01:16:06:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Aug/2021:16:06:44 +0000] \"POST /invocations HTTP/1.1\" 200 57 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')\n",
    "xgb_transformer.transform(manul_test_location, content_type='text/csv', split_type='Line')\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "957ee17e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-099005516989/sagemaker-xgboost-2021-08-01-16-01-11-021/manul_test.csv.out to ../data/manul_test.csv.out\n",
      "[0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir\n",
    "\n",
    "prediction = pd.read_csv(os.path.join(data_dir, 'manul_test.csv.out'), header=None)\n",
    "prediction = [round(num) for num in prediction.squeeze().values]\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d0579e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
